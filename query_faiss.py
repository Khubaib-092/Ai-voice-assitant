import sqlite3
import numpy as np
import os
import speech_recognition as sr
from faster_whisper import WhisperModel
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from googletrans import Translator
from playsound import playsound
import subprocess
import uuid
import time
import wave
import pyaudio
from gtts import gTTS
import threading
import random

# âœ… Base directory
base_dir = os.path.dirname(os.path.abspath(__file__))
fwhisper_model = WhisperModel("tiny", compute_type="int8", cpu_threads=4)

# ğŸ§  Load model and database
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
conn = sqlite3.connect(os.path.join(base_dir, "voice_dataset.db"))
cursor = conn.cursor()
cursor.execute('SELECT id, question, answer, answer_audio_path FROM qa_pairs')
rows = cursor.fetchall()
conn.close()

# ğŸ“¦ ID â†’ (question, answer, audio_path)
id_map = {
    i: (row[1], row[2], os.path.join(base_dir, row[3]) if not os.path.isabs(row[3]) else row[3])
    for i, row in enumerate(rows)
}

# ğŸ”  Embedding all questions
questions = [id_map[i][0] for i in id_map]
question_vecs = model.encode(questions)

# ğŸ—£ï¸ Natural filler phrases (to play during transcription)
FILLER_PHRASES = [
    "ÛÙ…Ù…Ù…... Ø§Ú†Ú¾Ø§ Ø³ÙˆØ§Ù„ ÛÛ’Û”",
    "Ø¬ÛŒ Ø¨Ø§Ù„Ú©Ù„ØŒ Ø§ÛŒÚ© Ù„Ù…Ø­ÛÛ”",
    "ÛŒÛ Ù…ÛŒÚº Ù†Û’ Ø³Ù†Ø§ØŒ Ø¯ÛŒÚ©Ú¾ØªÛŒ ÛÙˆÚºÛ”",
    "Ù¹Ú¾ÛŒÚ© ÛÛ’ØŒ ØªÚ¾ÙˆÚ‘Ø§ Ø³Ø§ Ø§Ù†ØªØ¸Ø§Ø± Ú©ÛŒØ¬ÛŒÛ’Û”",
    "Ø³Ù…Ø¬Ú¾ Ú¯Ø¦ÛŒØŒ Ø§Ø¨Ú¾ÛŒ Ø¨ØªØ§ØªÛŒ ÛÙˆÚºÛ”",
    "ÛŒÛ Ø°Ø±Ø§ ØºÙˆØ± Ø³Û’ Ø³Ù† Ø±ÛÛŒ ÛÙˆÚºÛ”",
    "Ø§ÛŒÚ© Ø³ÛŒÚ©Ù†ÚˆØŒ Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾ØªÛŒ ÛÙˆÚºÛ”",
    "ÛØ§Úº Ø¬ÛŒØŒ ÛŒÛ Ø¯Ú¾ÛŒØ§Ù† Ø³Û’ Ø³Ù† Ø±ÛÛŒ ÛÙˆÚºÛ”",
    "Ø¬ÛŒ Ø¶Ø±ÙˆØ±ØŒ Ø¬ÙˆØ§Ø¨ ØªÙ„Ø§Ø´ Ú©Ø± Ø±ÛÛŒ ÛÙˆÚºÛ”",
    "ØªÚ¾ÙˆÚ‘Ø§ Ø³Ø§ ÙˆÙ‚Øª Ø¯ÛŒÚºØŒ Ù…ÛŒÚº Ø¯ÛŒÚ©Ú¾ Ø±ÛÛŒ ÛÙˆÚºÛ”"
]

FILLER_AUDIO_FILES = []
filler_dir = os.path.join(base_dir, "filler_audios")
os.makedirs(filler_dir, exist_ok=True)

for i, phrase in enumerate(FILLER_PHRASES):
    audio_path = os.path.join(filler_dir, f"filler_{i}.mp3")
    if not os.path.exists(audio_path):
        tts = gTTS(text=phrase, lang="ur")
        tts.save(audio_path)
    FILLER_AUDIO_FILES.append(audio_path)

# ğŸ”Š Background thread: speaks filler while transcription runs
def speak_filler_while(flag):
    path = random.choice(FILLER_AUDIO_FILES)
    try:
        playsound(path)
    except Exception as e:
        print(f"âš ï¸ Filler playback error: {e}")
    finally:
        flag["running"] = False

def recognize_speech():
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("ğŸ™ï¸ Listening (Google STT)...")
        recognizer.adjust_for_ambient_noise(source, duration=0.5)
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=4)
            transcription = recognizer.recognize_google(audio, language="ur-PK")
            print(f"ğŸ—£ï¸ You said (Google STT): {transcription}")
            return transcription
        except sr.WaitTimeoutError:
            print("â±ï¸ Timeout, no speech")
        except sr.UnknownValueError:
            print("âŒ Google couldn't understand")
        except Exception as e:
            print(f"âŒ Google STT failed: {e}")
        return ""
    
def speak(text):
    tts = gTTS(text=text, lang="ur")
    path = os.path.join(base_dir, "temp_reply.mp3")
    tts.save(path)
    playsound(path)
    os.remove(path)

def contains_exit_word(text):
    words = ["Ø§Ù„Ù„Û Ø­Ø§ÙØ¸", "Ø®Ø§ØªÙ…", "Ø®ØªÙ…", "Ø¨Ù†Ø¯", "Ø¨Ø³", "Ú†Ù„ÛŒÚº", "Ø®Ø¯Ø§ Ø­Ø§ÙØ¸"]
    return next((w for w in words if w in text), None)

def wait_for_wake_word():
    recognizer = sr.Recognizer()
    while True:
        with sr.Microphone() as source:
            recognizer.adjust_for_ambient_noise(source, duration=0.5)
            print("ğŸ‘‚ Say 'ÛÛŒÙ„Ùˆ' or 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…' to begin...")
            try:
                audio = recognizer.listen(source, timeout=4, phrase_time_limit=5)
                wake_phrase = recognizer.recognize_google(audio, language="ur-PK")
                print(f"ğŸ—£ï¸ Heard: {wake_phrase}")
                if "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…" in wake_phrase:
                    speak("Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…ØŒ Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ú©ÛŒØ³Û’ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛŒ ÛÙˆÚºØŸ")
                    return
                if "ÛÛŒÙ„Ùˆ" in wake_phrase:
                    speak("ÛÛŒÙ„ÙˆØŒ Ù…ÛŒÚº Ø¢Ù¾ Ú©ÛŒ Ú©ÛŒØ³Û’ Ù…Ø¯Ø¯ Ú©Ø± Ø³Ú©ØªÛŒ ÛÙˆÚºØŸ")
                    return
            except sr.WaitTimeoutError:
                print("â±ï¸ No input, trying again...")
            except:
                print("âŒ Could not understand wake word.")

def search(query, top_k=3, min_score=0.3):
    if not query.strip():
        return

    translated = Translator().translate(query, dest="ur").text
    print(f"ğŸŒ Translated: {translated}")

    query_vec = model.encode([translated])
    scores = cosine_similarity(query_vec, question_vecs)[0]
    top_indices = np.argsort(scores)[::-1][:top_k]

    print("\nğŸ” Top Matches:")
    for idx in top_indices:
        score = scores[idx]
        print(f"ğŸ”¹ Score: {score:.2f}")
        if score < min_score:
            print("âš ï¸ Match skipped: below threshold")
            continue
        question, answer, audio_path = id_map[idx]
        print(f"â“ {question}")
        print(f"ğŸ’¬ {answer}")
        if os.path.exists(audio_path):
            try:
                playsound(audio_path)
            except Exception as e:
                print(f"âš ï¸ Couldn't play audio: {e}")
        else:
            print(f"âŒ Missing audio: {audio_path}")

if __name__ == "__main__":
    wait_for_wake_word()
    while True:
        query = recognize_speech()
        if not query:
            continue
        exit_word = contains_exit_word(query)
        if exit_word:
            speak(exit_word)
            break
        search(query)
